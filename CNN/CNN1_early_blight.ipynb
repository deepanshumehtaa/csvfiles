{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62661,
     "status": "ok",
     "timestamp": 1585232917281,
     "user": {
      "displayName": "Deepânshu Mėhtá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3_6fjGsftK78XKz5YtXFmx4hk_2_XXLeb2ahOqg=s64",
      "userId": "18446494080185829441"
     },
     "user_tz": -330
    },
    "id": "tv9AXVnRNOKM",
    "outputId": "fc0329da-aa85-42cf-c12d-58dbaa686fe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\n\\n# File Paths\\n\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\n\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n\\n\\n!mkdir -p drive\\n!google-drive-ocamlfuse drive\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install keras --upgrade\n",
    "''''''''''\n",
    "\n",
    "# File Paths\n",
    "\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "'''''''''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWOZTQjNPJFJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "traindir = 'train'\n",
    "testdir = 'test'\n",
    "\n",
    "#know your Data\n",
    "\n",
    "train_b_dir = os.path.join(traindir, 'Blight')  # directory with our training cat pictures\n",
    "train_h_dir = os.path.join(traindir, 'Healthy')  # directory with our training dog pictures\n",
    "\n",
    "validation_b_dir = os.path.join(testdir, 'Blight')  # directory with our validation cat pictures\n",
    "validation_h_dir = os.path.join(testdir, 'Healthy')  # directory with our validation dog pictures\n",
    "\n",
    "num_b_tr = len(os.listdir(train_b_dir))\n",
    "num_h_tr = len(os.listdir(train_h_dir))\n",
    "\n",
    "num_b_val = len(os.listdir(validation_b_dir))\n",
    "num_h_val = len(os.listdir(validation_h_dir))\n",
    "\n",
    "total_train = num_b_tr + num_h_tr\n",
    "total_val = num_b_val + num_h_val\n",
    "\n",
    "print(total_train)\n",
    "print(total_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4f6I69WHr51"
   },
   "source": [
    "# Potato_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7936,
     "status": "ok",
     "timestamp": 1585113991869,
     "user": {
      "displayName": "Deepânshu Mėhtá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3_6fjGsftK78XKz5YtXFmx4hk_2_XXLeb2ahOqg=s64",
      "userId": "18446494080185829441"
     },
     "user_tz": -330
    },
    "id": "U2ANnfPDGVRH",
    "outputId": "43145400-e621-43ac-f6b6-2b1eb8556e5b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#CNN\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjEShS6WHWCr"
   },
   "source": [
    "*Preparing Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOLcHymBHKzd"
   },
   "outputs": [],
   "source": [
    "# Building the Model \n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (64, 64, 3),  kernel_initializer=tf.keras.initializers.Zeros()))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.Dense(2)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqqvYLlgTLU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FkcWQI9hTOkm"
   },
   "outputs": [],
   "source": [
    "# prepared by Expert\n",
    "\n",
    "rms = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(optimizer = rms, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "\n",
    "\n",
    "#prepared by me:\n",
    "\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3rqfRdOHnyQ"
   },
   "source": [
    "*Data Augmentation*\n",
    "\n",
    "* Shearing of images\n",
    "* Random zoom\n",
    "* Horizontal flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GM2nNaPfHpAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.1,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Training Set\n",
    "train_set = train_datagen.flow_from_directory(traindir,\n",
    "                                             target_size=(64,64),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "#Test Set /no output available\n",
    "test_set = test_datagen.flow_from_directory(testdir,\n",
    "                                            target_size=(64,64),\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_rI_M-OT4P1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Batch Shape (32, 64, 64, 3)\n",
      "Labels Batch Shape (32,)\n"
     ]
    }
   ],
   "source": [
    "# showing the output for the above code\n",
    "\n",
    "for data_batch, labels_batch in train_set:\n",
    "  print('Data Batch Shape', data_batch.shape)\n",
    "  print('Labels Batch Shape', labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlTkBpqSJUBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6930 - acc: 0.5060"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (32, 2) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-985842868227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_val\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m           mode='test')\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1242\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m-> 1244\u001b[1;33m         x, y, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2438\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2439\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2440\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2441\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2442\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    511\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m                            \u001b[1;34m'This loss expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                            \u001b[1;34m'targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (32, 2) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "# Time for Training:\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "epochs=100\n",
    "history = model.fit_generator(\n",
    "    train_set,\n",
    "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#history = model.fit_generator(train_set,\n",
    "#                         steps_per_epoch=10,\n",
    "#                         epochs = 30,\n",
    "#                         validation_data = test_set,\n",
    "#                         validation_steps = 5, \n",
    "                        #callbacks=[tensorboard]\n",
    "#                        );\n",
    "\n",
    "# steps_per_epoch = 2000/20 = 100, validation_steps = 1000/20 = 50\n",
    "\n",
    "\n",
    "\n",
    "#history = model.fit_generator(train_set, steps_per_epoch = 100, epochs = 30, validation_data = test_set, validation_steps = 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YAEFy6yUn4z"
   },
   "source": [
    "# Accuracy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2tWbBoZJUyq"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "test_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Plotting Training and Testing Loss\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, test_loss, 'b', label='Testing Loss')\n",
    "plt.title('Training and validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vscWouEoU4cw"
   },
   "outputs": [],
   "source": [
    "# Plotting Training and Testing Accuracy\n",
    "\n",
    "plt.plot(epochs, acc, 'ko', label='Training acc')\n",
    "plt.plot(epochs, test_acc, 'k', label='Testing acc')\n",
    "plt.title('Training and Testing accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pYWnzEi7VHJz"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(150, 150), batch_size=20, class_mode='binary')\n",
    "test_loss, test_acc = model1.evaluate_generator(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNo52gy6z93QLmKSxr2fBln",
   "collapsed_sections": [
    "6YAEFy6yUn4z"
   ],
   "name": "CNN1_early_blight.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
